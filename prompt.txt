using deepthink in ppo_agent.py: """"
PPO Trading Environment
=======================

Custom Gymnasium environment tailored for training and evaluating PPO agents
on financial time series (OHLCV) data.

This implementation emphasizes robustness and reproducibility by:
 - validating input data early and clearly
 - exposing an observation signature (obs_signature) that can be persisted and
   used by model save/load utilities to detect feature/lookback drift
 - providing clear, documented behavior for action/observation spaces
 - separating feature extraction and reward calculation (pluggable)
 - ensuring deterministic shapes (padding the lookback window) so observation
   vectors are stable across episodes

Design notes / rationale (deep thinking summary):
 - The agent, VecNormalize, and saved policy weights must agree on the
   observation dimensionality. To make that explicit we provide obs_signature()
   which returns (lookback_window, n_features, obs_dim, feature_names, ...).
   The model-saving logic should persist this signature (obs_config.yaml)
   and the loading logic should validate it against the environment used for
   evaluation/backtesting. This prevents silent shape mismatches and cryptic
   PyTorch "size mismatch" errors.
 - The environment expects a dataframe with standard OHLCV columns. Any
   technical indicator generation happens in StateFeatureExtractor (pluggable),
   which returns processed_data with columns corresponding to features used
   by the policy.
 - We avoid creating "fake" or empty environments for model loading: if the
   caller needs to load vecnormalize stats they must provide a real env that
   matches the obs_signature saved with the model.
 - Observation layout (flattened) is: [ market_window_features_flattened, portfolio_features ],
   where portfolio_features has fixed length = 5. This ordering must be honored
   by the agent's feature extractor (TradingFeatureExtractor).

Author: PPO Trading System
"""

from __future__ import annotations

import math
from typing import Dict, List, Tuple, Any, Optional

import gymnasium as gym
from gymnasium import spaces
import numpy as np
import pandas as pd
from loguru import logger
import yaml

from .state_features import StateFeatureExtractor
from .reward_functions import RewardCalculator


class TradingEnvironment(gym.Env):
    """
    High-level behavior:
    - The environment maintains a processed_data DataFrame produced by the
    StateFeatureExtractor. processed_data must have no NaNs and supply at least
    `lookback_window` rows for the environment to operate.
    - At each step the agent receives a flattened observation consisting of:
        [ flattened lookback_window x n_features market block, 5 portfolio stats ]
    The last 5 entries are the portfolio features:
        [ normalized_balance, position_size, normalized_unrealized_pnl, normalized_total_trades, current_drawdown ]
    - Action space can be discrete (e.g. Hold/Buy/Sell) or continuous (target position).
    - The environment computes rewards using a pluggable RewardCalculator.
    - The environment exposes obs_signature() for persistence/validation.
    """

    METADATA = {"render_modes": ["human"]}

    def __init__(self,
                data: pd.DataFrame,
                config_path: str = "config/model_config.yaml",
                initial_balance: float = 10_000.0,
                transaction_cost: float = 0.0001,
                render_mode: Optional[str] = None,
                expected_signature: Optional[Dict[str, Any]] = None):
        super().__init__()

        # Load configuration
        with open(config_path, "r") as f:
            self.config = yaml.safe_load(f)

        # Copy input to avoid side effects
        self.raw_data = data.copy() if data is not None else pd.DataFrame()
        self.initial_balance = float(initial_balance)
        self.transaction_cost = float(transaction_cost)
        self.render_mode = render_mode

        # Environment-specific params from config
        env_cfg = self.config.get("environment", {})
        self.lookback_window: int = int(env_cfg.get("lookback_window", 50))
        self.action_config: Dict[str, Any] = env_cfg.get("action_space", {"type": "discrete", "actions": ["hold","buy","sell"]})
        self.reward_config: Dict[str, Any] = env_cfg.get("reward_function", {})

        # Backtesting/risk config (used by position sizing & risk checks)
        self.backtest_cfg: Dict[str, Any] = self.config.get("backtesting", {})

        # Feature extractor & reward calculator (pluggable)
        self.feature_extractor = StateFeatureExtractor(self.config)
        self.reward_calculator = RewardCalculator(self.config)

        # keep expected_signature (optional) to perform a crisp validation during init
        self._expected_signature = expected_signature

        # Process data and validate; raises ValueError if requirements not met
        self._prepare_data()

        # Build spaces based on processed_data and lookback window
        self._setup_spaces()

        # reset internal state
        self.reset()

        logger.info(f"Trading environment initialized with {len(self.raw_data)} data points")

    # ------------------------------
    # Observation/signature helpers
    # ------------------------------
    def obs_signature(self) -> Dict[str, Any]:
        """
        Return a serializable signature describing the observation layout.
        Use this signature to persist alongside models, and validate during model loading.
        Example:
            {
                "lookback_window": 50,
                "n_features": 12,
                "feature_names": ["sma","rsi",...],
                "portfolio_features_dim": 5,
                "obs_dim": 605
            }
        """
        cols = list(self.processed_data.columns)
        return {
            "lookback_window": int(self.lookback_window),
            "n_features": int(len(cols)),
            "feature_names": cols,
            "portfolio_features_dim": 5,
            "obs_dim": int(self.lookback_window * len(cols) + 5)
        }

    # ------------------------------
    # Data preparation / validation
    # ------------------------------
    def _prepare_data(self) -> None:
        """
        Validate raw_data, compute features and drop NaNs.

        Raises:
            ValueError: if required OHLCV columns missing or not enough samples remain after indicators.
        """
        required_columns = ["open", "high", "low", "close", "volume"]
        if not isinstance(self.raw_data, pd.DataFrame):
            raise ValueError("data must be a pandas DataFrame")

        missing = [c for c in required_columns if c not in self.raw_data.columns]
        if missing:
            raise ValueError(f"Data must contain columns: {required_columns}. Missing: {missing}")

        # Build processed_data using the pluggable StateFeatureExtractor
        self.processed_data = self.feature_extractor.extract_features(self.raw_data)

        # After feature extraction, drop NaNs (indicator warmup)
        self.processed_data = self.processed_data.dropna(axis=0, how="any").reset_index(drop=True)

        if len(self.processed_data) < self.lookback_window:
            raise ValueError(f"Insufficient data after feature extraction: need at least lookback_window ({self.lookback_window}) rows, got {len(self.processed_data)}")

        # cache number of features for consistent use across the env
        self.n_features = int(len(self.processed_data.columns))

        # Optional: validate expected signature (if provided by caller)
        if self._expected_signature:
            sig = self.obs_signature()
            # Compare shapes; tolerant to extra metadata but strict on obs_dim
            if sig.get("obs_dim") != self._expected_signature.get("obs_dim"):
                raise ValueError(
                    f"Observation signature mismatch. Expected obs_dim={self._expected_signature.get('obs_dim')}, "
                    f"but environment produces obs_dim={sig.get('obs_dim')}. This usually means the feature set or "
                    "lookback window changed since the model was saved."
                )

        logger.info(f"Data prepared: {len(self.processed_data)} samples, {len(self.processed_data.columns)} features")

    # ------------------------------
    # Spaces setup
    # ------------------------------
    def _setup_spaces(self) -> None:
        """
        Construct action_space and observation_space.
        Observation is a flat Box of length (lookback_window * n_features + n_portfolio_features).
        """
        # Build action space
        if self.action_config.get("type", "discrete") == "discrete":
            actions = list(self.action_config.get("actions", ["hold", "buy", "sell"]))
            self.action_space = spaces.Discrete(len(actions))
        else:
            # Continuous action: single target position scalar in [min, max]
            bounds = self.action_config.get("continuous_bounds", {"position_size": [-1.0, 1.0]})
            low = float(bounds["position_size"][0])
            high = float(bounds["position_size"][1])
            self.action_space = spaces.Box(low=np.array([low], dtype=np.float32), high=np.array([high], dtype=np.float32), dtype=np.float32)

        # Observation space
        # Use the actual feature count produced by the feature extractor
        n_features = getattr(self, "n_features", None)
        if n_features is None:
            # fallback to processed_data if n_features wasn't cached for some reason
            n_features = len(self.processed_data.columns)

        n_portfolio = 5  # fixed layout: normalized_balance, position, normalized_unrealized_pnl, normalized_total_trades, current_drawdown
        obs_dim = int(self.lookback_window * n_features + n_portfolio)

        # Use unbounded Box; normalization handled externally (VecNormalize)
        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(obs_dim,), dtype=np.float32)

        logger.info(f"Action space: {self.action_space}")
        logger.info(f"Observation space: {self.observation_space.shape}")

    # ------------------------------
    # Core gym methods
    # ------------------------------
    def reset(self, seed: Optional[int] = None, options: Optional[Dict] = None) -> Tuple[np.ndarray, Dict]:
        """
        Reset environment state and return initial observation + info.

        Returns:
            observation (np.ndarray): flattened observation vector
            info (dict): dictionary with initial portfolio metadata
        """
        super().reset(seed=seed)

        # Start simulation at index = lookback_window (first index where a full window exists)
        self.current_step: int = int(self.lookback_window)
        self.balance: float = float(self.initial_balance)
        self.position: float = 0.0
        self.position_entry_price: float = 0.0
        self.total_trades: int = 0
        self.winning_trades: int = 0
        self.trade_history: List[Dict[str, Any]] = []
        self.portfolio_values: List[float] = [self.initial_balance]
        self.unrealized_pnl: float = 0.0

        # Tracking
        self.max_portfolio_value: float = self.initial_balance
        self.current_drawdown: float = 0.0
        self.max_drawdown: float = 0.0

        obs = self._get_observation()
        info = self._get_info()
        return obs, info

    def step(self, action: Any) -> Tuple[np.ndarray, float, bool, bool, Dict]:
        """
        Execute action and advance simulation by one time-step.

        Returns:
            observation, reward, terminated, truncated, info
        """
        # Validate action if necessary (gym will typically validate)
        self._execute_action(action)

        # advance time
        self.current_step += 1

        # compute reward using pluggable RewardCalculator
        reward = self.reward_calculator.calculate_reward(
            current_price=self._get_current_price(),
            position=self.position,
            balance=self.balance,
            unrealized_pnl=self.unrealized_pnl,
            drawdown=self.current_drawdown,
            portfolio_value=self._get_portfolio_value()
        )

        # update portfolio tracking details
        self._update_portfolio_tracking()

        # termination/truncation checks
        terminated = (self.current_step >= (len(self.processed_data) - 1))
        truncated = self._check_early_termination()

        obs = self._get_observation()
        info = self._get_info()
        return obs, float(reward), bool(terminated), bool(truncated), info

    # ------------------------------
    # Actions and position management
    # ------------------------------
    def _execute_action(self, action: Any) -> None:
        """
        Interpret and execute an action.

        Discrete action expects integer index (0..n-1) mapping to actions in config.
        Continuous action expects a scalar (or array-like) representing target position.
        """
        current_price = self._get_current_price()

        if self.action_config.get("type", "discrete") == "discrete":
            # ensure action is int and within action_space
            if not isinstance(action, (int, np.integer)):
                # Some wrappers (VecEnv) may pass np.ndarray; handle gracefully
                if isinstance(action, (list, tuple, np.ndarray)):
                    action = int(np.asarray(action).flatten()[0])
                else:
                    action = int(action)

            actions = self.action_config.get("actions", ["hold", "buy", "sell"])
            idx = int(action) % len(actions)
            action_name = actions[idx]

            if action_name == "buy" and self.position <= 0:
                self._open_position(1.0, current_price)
            elif action_name == "sell" and self.position >= 0:
                self._open_position(-1.0, current_price)
            elif action_name == "hold":
                # update unrealized PnL only
                if self.position != 0.0:
                    self.unrealized_pnl = (current_price - self.position_entry_price) * self.position
        else:
            # continuous: target position scalar
            # Accept array-like, convert to float
            if isinstance(action, (list, tuple, np.ndarray)):
                target_position = float(np.asarray(action).flatten()[0])
            else:
                target_position = float(action)
            if not math.isfinite(target_position):
                return
            if target_position != self.position:
                self._adjust_position(target_position, current_price)

    def _open_position(self, direction: float, price: float) -> None:
        """
        Open a new position with direction +/-1. Closes existing position first.

        Position sizing is determined by config['backtesting']['position_sizing'].
        """
        if self.position != 0.0:
            self._close_position(price)

        sizing = self.backtest_cfg.get("position_sizing", {"method": "fixed_fraction", "fraction": 0.01, "max_position": 0.1})

        if sizing.get("method", "fixed_fraction") == "fixed_fraction":
            fraction = float(sizing.get("fraction", 0.01))
            position_value = self.balance * fraction
            position_size = (position_value / price) * float(direction)
        elif sizing.get("method") == "fixed":
            position_size = float(sizing.get("size", 0.01)) * float(direction)
        else:
            position_size = 0.01 * float(direction)

        # cap by max position
        max_pos = float(sizing.get("max_position", 0.1))
        max_pos_val = self.balance * max_pos
        max_pos_size = max_pos_val / price if price > 0 else max_pos_val
        if abs(position_size) > max_pos_size:
            position_size = math.copysign(max_pos_size, position_size)

        trade_cost = abs(position_size) * price * self.transaction_cost

        # Ensure sufficient budget for trade cost -> otherwise ignore
        if self.balance >= trade_cost:
            self.position = float(position_size)
            self.position_entry_price = float(price)
            self.balance -= float(trade_cost)
            self.total_trades += 1
            self.unrealized_pnl = 0.0

            self.trade_history.append({
                "step": self.current_step,
                "action": "BUY" if direction > 0 else "SELL",
                "size": self.position,
                "price": price,
                "cost": trade_cost,
                "balance_after": self.balance
            })

    def _close_position(self, price: float) -> None:
        """Close current position and update balance, PnL and trade logs."""
        if self.position == 0.0:
            return

        pnl = (price - self.position_entry_price) * self.position
        trade_cost = abs(self.position) * price * self.transaction_cost
        net_pnl = pnl - trade_cost

        # If position was a size in units (not fraction), adjust balance accordingly:
        # adding the proceeds and net pnl. This assumes position is in units of asset.
        self.balance += self.position * price + net_pnl

        if net_pnl > 0:
            self.winning_trades += 1

        self.trade_history.append({
            "step": self.current_step,
            "action": "CLOSE",
            "size": self.position,
            "entry_price": self.position_entry_price,
            "exit_price": price,
            "pnl": net_pnl,
            "balance_after": self.balance
        })

        # reset position
        self.position = 0.0
        self.position_entry_price = 0.0
        self.unrealized_pnl = 0.0

    def _adjust_position(self, target_position: float, price: float) -> None:
        """
        Adjust position to match target_position (continuous action).
        Applies trade cost to the change in position.
        """
        if target_position == self.position:
            return

        delta = target_position - self.position
        trade_cost = abs(delta) * price * self.transaction_cost
        if self.balance >= trade_cost:
            # update weighted average entry price if adding to a position
            if self.position == 0.0 and target_position != 0.0:
                self.position_entry_price = price
            elif self.position != 0.0 and target_position != 0.0:
                # compute new average entry price (naive weighted approach)
                prev_val = self.position * self.position_entry_price
                added_val = delta * price
                new_pos = self.position + delta
                if new_pos != 0:
                    self.position_entry_price = (prev_val + added_val) / new_pos
                else:
                    self.position_entry_price = 0.0

            self.position = float(target_position)
            self.balance -= float(trade_cost)
            self.total_trades += 1

    # ------------------------------
    # Pricing and portfolio helpers
    # ------------------------------
    def _get_current_price(self) -> float:
        """Return the close price at current_step (float)."""
        # assume processed_data includes a "close" column; use safe access to avoid KeyError
        try:
            return float(self.processed_data.iloc[self.current_step]["close"])
        except Exception:
            # fallback: use last column if "close" missing
            return float(self.processed_data.iloc[self.current_step].iloc[-1])

    def _get_portfolio_value(self) -> float:
        """Compute current portfolio value (balance + position * current_price)."""
        current_price = self._get_current_price()
        position_value = self.position * current_price if self.position != 0.0 else 0.0
        return float(self.balance + position_value)

    def _update_portfolio_tracking(self) -> None:
        """Update portfolio value history and drawdowns."""
        pv = self._get_portfolio_value()
        self.portfolio_values.append(pv)

        if pv > self.max_portfolio_value:
            self.max_portfolio_value = pv

        # avoid division by zero
        if self.max_portfolio_value > 0:
            self.current_drawdown = (self.max_portfolio_value - pv) / float(self.max_portfolio_value)
        else:
            self.current_drawdown = 0.0

        if self.current_drawdown > self.max_drawdown:
            self.max_drawdown = self.current_drawdown

    def _check_early_termination(self) -> bool:
        """Return True if the episode should be truncated early for risk rules."""
        risk_cfg = self.backtest_cfg.get("risk_management", {})
        max_drawdown_stop = float(risk_cfg.get("max_drawdown_stop", 0.5))
        if self.current_drawdown > max_drawdown_stop:
            return True

        # Minimum balance stop (10% of initial capital default)
        if self.balance < (0.1 * self.initial_balance):
            return True

        return False

    # ------------------------------
    # Observation & info
    # ------------------------------
    def _get_observation(self) -> np.ndarray:
        """
        Construct the flattened observation for the current_step.

        Layout:
        - market_features_window: shape (lookback_window, n_features) -> flattened
        - portfolio_features: length 5
        """
        # compute window start (may be <0 if current_step < lookback_window; but our reset ensures current_step >= lookback_window)
        start_idx = max(0, self.current_step - self.lookback_window)
        window = self.processed_data.iloc[start_idx:self.current_step]

        # pad if near start (should not happen after reset but keep safe)
        if len(window) < self.lookback_window:
            n_missing = self.lookback_window - len(window)
            pad_shape = (n_missing, len(self.processed_data.columns))
            pad = np.zeros(pad_shape, dtype=np.float32)
            window_vals = np.vstack([pad, window.values.astype(np.float32)])
        else:
            window_vals = window.values.astype(np.float32)

        flat_market = window_vals.flatten()

        # portfolio features (normalized where appropriate)
        portfolio_value = self._get_portfolio_value()
        portfolio_features = np.array([
            self.balance / float(self.initial_balance),                # normalized balance
            self.position,                                             # position (signed)
            (self.unrealized_pnl / float(self.initial_balance)) if self.initial_balance != 0 else 0.0,
            float(self.total_trades) / 1000.0,                         # scaled trade count
            float(self.current_drawdown)
        ], dtype=np.float32)

        obs = np.concatenate([flat_market.astype(np.float32), portfolio_features], axis=0)

        # quick sanity check: ensure obs length matches observation_space shape
        try:
            expected_len = int(self.observation_space.shape[0])
            if obs.shape[0] != expected_len:
                raise ValueError(f"Observation length mismatch: got {obs.shape[0]}, expected {expected_len}")
        except Exception:
            # if observation_space not yet set or any error, skip strict check
            pass

        return obs

    def _get_info(self) -> Dict[str, Any]:
        """Return a dictionary of auxiliary metrics for monitoring / callbacks."""
        pv = self._get_portfolio_value()
        win_rate = (float(self.winning_trades) / float(max(1, self.total_trades))) if self.total_trades > 0 else 0.0
        return {
            "step": int(self.current_step),
            "balance": float(self.balance),
            "position": float(self.position),
            "portfolio_value": float(pv),
            "total_return": float((pv - self.initial_balance) / float(self.initial_balance)),
            "total_trades": int(self.total_trades),
            "winning_trades": int(self.winning_trades),
            "win_rate": float(win_rate),
            "max_drawdown": float(self.max_drawdown),
            "current_drawdown": float(self.current_drawdown),
            "unrealized_pnl": float(self.unrealized_pnl)
        }

    # ------------------------------
    # Rendering
    # ------------------------------
    def render(self) -> None:
        """Minimal human-friendly render output."""
        if self.render_mode == "human":
            info = self._get_info()
            print(
                f"[Step {info['step']}] Balance: ${info['balance']:.2f} | "
                f"Portfolio: ${info['portfolio_value']:.2f} | Return: {info['total_return']:.2%} | "
                f"Drawdown: {info['current_drawdown']:.2%} | Position: {info['position']:.4f}"
            )

    # ------------------------------
    # Utility constructors
    # ------------------------------
    @classmethod
    def build_for_model_loading(cls,
                                data: pd.DataFrame,
                                config_path: str,
                                expected_signature: Dict[str, Any]) -> "TradingEnvironment":
        """
        Convenience constructor used by model-loading code: it enforces that the env
        being created will match the provided expected_signature (obs_config saved
        alongside the model). It raises a clear ValueError if the signature mismatches.
        """
        env = cls(data=data, config_path=config_path, expected_signature=expected_signature)
        sig = env.obs_signature()
        if sig.get("obs_dim") != expected_signature.get("obs_dim"):
            raise ValueError(
                f"obs_dim mismatch building env_for_model_loading: expected {expected_signature.get('obs_dim')}, got {sig.get('obs_dim')}"
            )
        return env
", i'm saving model, with new set of yaml files: [obs_config.yaml: "feature_names:
- open
- high
- low
- close
- volume
- sma_5
- sma_20
- sma_50
- ema_12
- ema_26
- rsi_14
- macd
- macd_signal
- bb_upper
- bb_lower
- atr_14
- hour_of_day
- day_of_week
- is_weekend
- price_change
- high_low_ratio
- volume_ratio
- volatility
- price_position
- hour_sin
- hour_cos
- dow_sin
- dow_cos
lookback_window: 100
n_features: 28
obs_dim: 2805
portfolio_features_dim: 5
", policy_kwargs.yaml: "activation_fn: relu
features_extractor_class: TradingFeatureExtractor
features_extractor_kwargs:
  portfolio_features_dim: 5
net_arch:
- 256
- 256
ortho_init: true
other: {}
version: 1
"] how do I update these to use model_config.yaml: "# PPO Model Configuration
# ========================

# PPO Algorithm Parameters
ppo:
  # Core PPO Settings
  learning_rate: 0.0003
  n_steps: 2048
  batch_size: 64
  n_epochs: 10
  gamma: 0.99  # Discount factor
  gae_lambda: 0.95  # GAE parameter
  clip_range: 0.2
  clip_range_vf: null
  normalize_advantage: true
  ent_coef: 0.0  # Entropy coefficient
  vf_coef: 0.5  # Value function coefficient
  max_grad_norm: 0.5
  
  # Network Architecture
  policy_kwargs:
    net_arch: [256, 256]  # Hidden layers for both actor and critic
    activation_fn: "relu"
    ortho_init: true
    
# Training Configuration
training:
  total_timesteps: 10000
  eval_freq: 10000
  n_eval_episodes: 10
  save_freq: 50000
  
  # Early Stopping
  early_stopping:
    enabled: true
    patience: 20
    min_improvement: 0.01
    
  # Curriculum Learning
  curriculum:
    enabled: false
    stages:
      - timesteps: 200000
        difficulty: "easy"
      - timesteps: 500000
        difficulty: "medium"
      - timesteps: 1000000
        difficulty: "hard"

# Environment Configuration
environment:
  # State Space (Features)
  features:
    price_features:
      - "open"
      - "high" 
      - "low"
      - "close"
      - "volume"
    
    technical_indicators:
      - "sma_5"
      - "sma_20"
      - "sma_50"
      - "ema_12"
      - "ema_26"
      - "rsi_14"
      - "macd"
      - "macd_signal"
      - "bb_upper"
      - "bb_lower"
      - "atr_14"
    
    time_features:
      - "hour_of_day"
      - "day_of_week"
      - "is_weekend"
      - "hour_sin"
      - "hour_cos"
      - "dow_sin"
      - "dow_cos"
    
  lookback_window: 100  # Number of previous timesteps to include
  
  # Action Space
  action_space:
    type: "discrete"  # "discrete" or "continuous"
    actions:
      - "hold"
      - "buy"
      - "sell"
    # For continuous: action bounds
    continuous_bounds:
      position_size: [-1.0, 1.0]  # -1 = full short, +1 = full long
      
  # Reward Function
  reward_function:
    type: "profit_based"  # "profit_based", "sharpe_based", "custom"
    parameters:
      profit_weight: 1.0
      penalty_weight: 0.1
      drawdown_penalty: 2.0
      transaction_cost: 0.0001  # 0.01%
      risk_free_rate: 0.02  # 2% annual
      
    # Custom reward components
    custom_rewards:
      trend_following: 0.1
      mean_reversion: 0.05
      volatility_targeting: 0.05

# Backtesting Configuration
backtesting:
  initial_balance: 10000.0
  transaction_costs:
    commission: 0.0001  # 0.01% per trade
    spread: 0.00005     # 0.005% spread
  
  # Position Sizing
  position_sizing:
    method: "fixed_fraction"  # "fixed", "fixed_fraction", "kelly", "volatility_target"
    fraction: 0.02  # 2% of equity per trade
    max_position: 0.1  # Maximum 10% of equity
    
  # Risk Management
  risk_management:
    stop_loss: 0.02  # 2% stop loss
    take_profit: 0.04  # 4% take profit
    trailing_stop: false
    max_drawdown_stop: 0.15  # Stop trading at 15% drawdown

# Model Validation
validation:
  cross_validation:
    enabled: true
    n_splits: 5
    test_size: 0.2
    
  walk_forward_analysis:
    enabled: true
    train_period: 252  # Days
    test_period: 63   # Days
    step_size: 21     # Days
    
  performance_metrics:
    - "total_return"
    - "sharpe_ratio" 
    - "max_drawdown"
    - "win_rate"
    - "profit_factor"
    - "calmar_ratio"
    - "sortino_ratio"
" instead?